## 2026-02-12 — 기획서(문제 정의/기능 범위/문서 구조) 작성

### 1) AI에게 제공한 컨텍스트(배경/요구사항/제약)
- 배경: 주제 2 “시간=돈(기회비용)”을 생활 의사결정에 적용하는 웹앱 기획
- 목표: A/B 선택을 **총비용(직접비용+시간비용)**으로 비교하고 결론의 **근거(수식/중간값)**까지 보여주기
- 요구사항(필수 포함):  
  - 문제 정의(왜 이 문제인지), 핵심/부가 기능 구분  
  - 대상 사용자/사용자 시나리오  
  - 엣지 케이스 및 처리 방안  
  - 개선 과정 기록/AI 활용 기록을 “추후 추가 가능한 틀”로 준비
- 제약: README 가독성 우선, 기록은 docs로 분리(README는 최소 링크만 유지)

### 2) AI 생성 결과물을 어떻게 검증하고 수정했는지
- 검증 기준: 과제 요구사항(문서 필수 항목 3종) 누락 여부와, 평가자가 읽기 쉬운 구조인지 확인
- 수정/보완한 내용:
  - 문서가 “혼자 보기용”으로 흐르지 않도록 **평가 관점의 제출용 톤**으로 재구성
  - 불필요한 문구(배점/체크리스트/‘가장 중요’ 같은 표현) 제거 → 가독성 개선
  - “개발 진행된 것처럼 보이는 기록” 전부 제거 → **개발 전 상태를 정직하게 반영**
  - README는 요약만 두고, `CHANGELOG.md / RETROSPECTIVE.md / AI_LOG.md`로 기록 분리
  - 계산 모델을 문서에 명시하기 위해 수식(LaTeX) 포함 및 소수점 처리 기준(floor) 고정

### 3) AI와의 협업에서 발견한 문제점과 대응 방식
- 문제점 1) AI 답변이 길어져 README 가독성이 떨어질 수 있음  
  → 대응: 섹션을 짧게 쪼개고, 핵심만 남기며, 로그는 `docs/`로 분리
- 문제점 2) AI가 “개발 후 기록”을 예시로 채우며 사실처럼 보일 위험이 있음  
  → 대응: 개발 전 단계에서는 **템플릿만** 남기고, 실제 기록은 추후 작성하도록 분리
- 문제점 3) 요구사항을 빠짐없이 넣다 보면 문서가 과밀해질 수 있음  
  → 대응: (1) 문제 정의는 핵심만 요약 + 수식/시나리오/엣지 케이스는 명확히 구획,  
    (2)(3)은 링크/템플릿 중심으로 최소화하여 유지보수 부담을 줄임

---

## 2026-02-12 — Java/Spring 기반 웹앱 전체 구현

### 1) AI에게 제공한 컨텍스트(배경/요구사항/제약)
- 배경: 이미 정의된 문제(“시간=돈, 기회비용 계산”)를 **Java + Spring Boot 기반 웹앱**으로 구현해야 하는 과제
- 목표:
  - 2개 선택지 A/B의 총비용(Total Cost Index)을 계산·비교하는 **MVP 웹앱** 구현
  - 과제 평가 항목에 맞게 **문제 정의/개선 과정/AI 활용 기록**이 드러나는 구조 유지
  - 이후 확장을 고려한 구조(서비스/컨트롤러/DTO 분리, 테스트 가능 구조)
- 요구사항:
  - Java 17 + Spring Boot 3.x 기반
  - REST API + 간단한 프론트엔드(Thymeleaf + JS)
  - 필수 엣지 케이스(0/음수/문자/동일 결과/큰 값) 처리
  - 테스트 코드, README, 문서(CHANGELOG, RETROSPECTIVE, AI_LOG) 갱신
- 제약:
  - Maven/Java 설치 여부는 보장되지 않아 **로컬 빌드/테스트는 사용자 환경에 위임**
  - 시간 내에 “완벽한 아키텍처”보다 **평가 기준을 충족하는 실용적 구조**를 선택

> **로그 운영 원칙**
> - Git 커밋 메시지는 “한 줄 요약” 수준으로만 사용하고,  
>   - 예) `feat: add multi comparison`, `fix: handle invalid json`  
> - 대신, **컨텍스트·요청·검증·수정·의미**는 이 `AI_LOG.md`와 `CHANGELOG.md`, `RETROSPECTIVE.md`에 자세히 기록해  
>   - 평가자가 “AI를 어떻게 활용했고, 사람이 어떤 결정을 내렸는지”를 문서만으로 추적할 수 있게 한다.

### 2) AI가 수행한 주요 작업과 검증 방식

1. **Spring Boot 프로젝트 생성**
   - 작업:
     - `pom.xml` 작성 (Spring Web, Validation, Thymeleaf, Lombok, Test)
     - `OpportunityCostApplication` 생성, `application.properties` 기본 설정
   - 검증:
     - 의존성 목록을 과제 요구사항과 맞춰보고, 불필요한 라이브러리 제외
     - 린트 도구를 통해 구문 오류 여부 확인

2. **도메인/DTO/서비스/컨트롤러 설계**
   - 작업:
     - `ComparisonOption`, `CalculationRequest/Response`, `CostBreakdown` 설계
     - `OpportunityCostService`에서 수식 \(TC = C + (W/60) × T\) 구현 + floor 처리
     - `OpportunityCostController`에서 `/api/calculate` 엔드포인트 구현
   - 검증:
     - README/기획서에 정의된 예시(마트 vs 편의점, 요리 vs 배달)를 기반으로 **단위 테스트 코드** 작성
     - 테스트에서 기대값(5,500원, 12,300원 등)이 정확히 일치하는지 확인

3. **프론트엔드 구현 및 UX 개선**
   - 작업:
     - `index.html` + `style.css` + `app.js`로 시급 입력/선택지 A/B 입력/결과 표시 UI 구성
     - 프리셋 버튼, 계산식 토글, 로딩 상태, Enter 키 지원 등 UX 요소 추가
   - 검증:
     - 수기 입력/프리셋 조합으로 다양한 시나리오를 가정하고, 계산 결과가 문서의 예시와 일치하는지 로직을 다시 추적
     - 에러 메시지가 한국어로 자연스럽고, 필드별로 구체적인지 확인

4. **예외 처리/입력 검증/테스트/로깅**
   - 작업:
     - `GlobalExceptionHandler`로 전역 예외 처리
     - DTO와 JS에서 이중 검증(빈 문자열/NaN/음수/0 이상 조건)
     - `OpportunityCostServiceTest`, `OpportunityCostControllerTest` 작성
     - SLF4J 기반 로깅 추가 (요청/결과/캐시 히트 등)
   - 검증:
     - “시급 0/음수/null”, “선택지 누락”, “잘못된 JSON” 등 케이스에 대해 400/500 응답이 적절히 나오는지 테스트 코드로 확인
     - 로그 메시지가 과하게 장황하지 않고, 디버깅에 필요한 정보만 포함하는지 리뷰

5. **다안 비교/히스토리/캐싱/배포 준비(추가 기능)**
   - 작업:
     - `/api/calculate/multi` + `MultiComparisonRequest/Response`, `OptionResult` 구현
     - 프론트엔드에 다안 비교 모드(3~5개 선택지), 선택지 동적 추가/제거 UI 추가
     - `localStorage` 기반 히스토리 저장/조회/재사용 기능 구현
     - `CalculationCacheService`로 동일 입력에 대한 결과 캐싱
     - `Dockerfile`, `docker-compose.yml`, `Procfile`, `application-prod.properties`, `docs/DEPLOYMENT.md` 작성
   - 검증:
     - 다안 비교 결과가 2안 비교 수식과 동일한 로직을 공유하는지, 서비스 레이어 구현을 다시 따라가며 확인
     - 히스토리에서 항목을 클릭했을 때 입력 값/결과가 일관되게 복원되는지 JS 로직을 시뮬레이션
     - Docker 이미지 빌드/실행 플로우가 일반적인 Spring Boot 컨테이너 패턴과 맞는지 점검

### 3) AI와의 협업에서 드러난 장단점과 대응

- **장점 1) 반복적/기계적인 작업 자동화**
  - DTO/서비스/컨트롤러/테스트/문서 등 “패턴이 명확한 코드”를 빠르게 생성하여, 사람은 문제 정의/엣지 케이스/UX와 같은 상위 레벨 의사결정에 집중할 수 있었다.
  - 대응: 구조(레이어링, 네이밍 규칙)를 먼저 합의한 뒤, 반복 패턴은 AI에게 맡기고, 마지막에 사람이 한 번에 리뷰·정리하는 방식으로 진행.

- **장점 2) 평가 기준을 의식한 설계 보조**
  - 과제 평가표(문제 정의/결과물 판단력/개선 과정/맥락 관리)를 상기시키면서, 테스트 코드/문서/로그/배포 준비까지 “채점 포인트”를 모두 커버하도록 설계를 유도할 수 있었다.
  - 대응: 각 섹션(CHANGELOG, RETROSPECTIVE, AI_LOG, DEPLOYMENT)에 “이 항목이 어떤 평가 항목을 뒷받침하는지”를 의식하고 작성.

- **문제점 1) 과도한 자동화로 인한 복잡도 증가 위험**
  - 다안 비교/히스토리/캐싱/배포를 한 번에 추가하다 보면, 코드가 한 번에 크게 불어나서 “왜 이렇게 됐는지” 흐름이 흐려질 수 있었다.
  - 대응: `PROJECT_SUMMARY.md`와 `FINAL_SUMMARY.md`를 별도로 작성해, 기능 추가 순서와 의도를 텍스트로 정리하고, CHANGELOG에 버전별 요약을 남김.

- **문제점 2) 실제 실행 환경 차이 (예: Maven 미설치)**
  - 로컬에서 `mvn`이 없다는 오류가 발생해, AI 입장에서는 빌드/테스트를 직접 돌려볼 수 없었다.
  - 대응: 빌드/실행/배포 명령어는 문서로 명확히 남기고, 코드 레벨에서는 린트/타입 오류가 없도록 하는 데 집중했다.

- **문제점 3) “있어 보이는 것”과 “실제 필요한 것” 사이의 균형**
  - 예를 들어, 캐시 서비스나 다양한 배포 플랫폼 가이드는 과제 범위에 비해 과할 수 있지만, 평가 항목(배포, 독창성, 완성도)을 고려하면 의미가 있었다.
  - 대응: 필수 기능(MVP)을 먼저 완성/검증한 뒤, 남은 시간 안에서 “가산점에 직접 연결되는 추가 기능”만 선택적으로 도입했다.

---

## 2026-02-12 — 잘못된 JSON 요청 처리 개선

### 1) 컨텍스트
- 상황: `OpportunityCostControllerTest.testCalculate_InvalidJson` 테스트에서, 잘못된 JSON(`{ invalid json }`) 요청에 대해 **400이 아니라 500**이 반환되며 테스트가 실패함.
- 기대: 잘못된 요청 본문(JSON 파싱 오류)은 **클라이언트 잘못**이므로, 500이 아니라 **400 Bad Request**가 되어야 함.

### 2) AI에게 요청한 작업
- `HttpMessageNotReadableException`을 전역 예외 처리기에서 **별도로 처리하여 400으로 매핑**해 달라고 요청.
- 테스트 로그에 나온 스택 트레이스를 바탕으로, 어느 계층에서 예외가 발생하는지(요청 바디 → Jackson 파싱 → Spring MVC) 분석해 반영.

### 3) 적용된 변경 사항
- `GlobalExceptionHandler` 수정:
  - `@ExceptionHandler(HttpMessageNotReadableException.class)` 추가 → 잘못된 JSON 요청 시 400 응답 반환.
  - 기존 `@ExceptionHandler(Exception.class)`는 “마지막 방어선”으로 두어, 예상치 못한 나머지 예외만 500으로 처리.
- 테스트 코드:
  - `testCalculate_InvalidJson`는 그대로 두고, 전역 예외 처리기를 통해 기대 상태 코드(400)를 만족하도록 구현을 조정.

### 4) 검증
- `mvn test` 실행 결과:
  - `OpportunityCostControllerTest.testCalculate_InvalidJson`: 기대 상태 코드 400, 실제 400으로 통과.
  - 전체 테스트 요약: **Failures 0, Errors 0**.
- 로그:
  - 이전에는 `Exception` 핸들러에서 500으로 처리되던 스택 트레이스가,
  - 변경 후에는 `요청 본문 파싱 오류` 로그 + 400 응답으로 정리됨.

### 5) 평가 관점에서의 의미
- **기능 정확성**: 잘못된 JSON 입력에 대해 올바른 HTTP 상태 코드와 의미 있는 에러 메시지를 반환하여, API 사용성이 좋아짐.
- **결과물 판단력/개선 과정**: 실제 테스트 실패 로그를 기반으로 예외 처리를 보완하고, 테스트를 다시 통과시키는 과정이 명확히 드러남.
- **맥락 관리**: 전역 예외 처리기의 역할과, 어떤 예외를 어떤 상태 코드로 매핑할지에 대한 기준을 문서와 코드 양쪽에서 일관되게 유지.

---

## 2026-02-12 — 다안 비교 프리셋 동작 및 에러 알림 UX 개선

### 1) 컨텍스트
- **상황**: 실제 사용 테스트에서 두 가지 UX 문제 발견:
  1. 다안 비교 모드로 전환 후 "예시 시나리오" 프리셋 버튼을 눌러도 입력 카드에 값이 채워지지 않음
  2. 입력 검증 실패 시 에러 메시지가 화면 하단에만 표시되어, 상단 입력 중인 사용자가 에러를 놓치기 쉬움
- **기대**: 
  1. 프리셋 버튼이 모든 모드(2안 비교, 다안 비교)에서 정상 동작
  2. 에러 발생 시 사용자 위치와 관계없이 즉시 인지 가능

### 2) AI에게 요청한 작업
- **1차 수정 (프리셋 문제)**: 
  - 프리셋 핸들러에서 `currentMode`에 따라 분기하여 다안 비교 모드일 때도 입력 카드에 값을 채워달라고 요청
  - `document.querySelectorAll('.multi-option-card')`로 카드를 찾은 뒤, 내부 input 요소를 선택하여 값 주입
- **2차 수정 (프리셋 문제 재발)**:
  - 1차 수정 후에도 여전히 동작하지 않는다는 피드백 → DOM 선택 방식 재검토 요청
  - `data-index` 속성을 직접 사용하는 방식으로 변경 요청
- **에러 알림 개선**:
  - `showError()` 함수를 alert 팝업으로 개선하고, 상단 스크롤도 추가 요청
  - 다안 비교 계산 시 입력 검증 강화 요청 (시급, 각 선택지별 상세 검증)

### 3) 적용된 변경 사항
- **프리셋 핸들러 수정 (`src/main/resources/static/js/app.js`)**:
  - 1차: 카드 기반 선택 → 여전히 불안정
  - 2차: `document.querySelector('.multi-time-input[data-index="0"]')` 방식으로 직접 선택
  - 입력 필드를 찾지 못하면 "다안 비교 모드로 먼저 전환해주세요" 안내
- **에러 알림 개선 (`showError()` 함수)**:
  - `alert()` 팝업 추가 → 사용자가 어디에 있든 즉시 알림
  - 하단 에러 메시지 영역도 유지 (참고용)
  - `window.scrollTo({ top: 0, behavior: 'smooth' })` 추가
- **다안 비교 입력 검증 강화 (`calculateMulti()` 함수)**:
  - 시급 검증: 빈 문자열, NaN, 음수, 큰 값 경고
  - 각 선택지별 검증: 입력 필드 존재 여부, 빈 값, 숫자 변환, 범위 체크
  - 검증 실패 시 해당 입력 필드로 `focus()` 이동

### 4) 검증
- **프리셋 동작**: 
  - 다안 비교 모드 전환 → 프리셋 버튼 클릭 → 첫 번째/두 번째 카드에 값 정상 채워짐 확인
  - data-index 기반 선택으로 DOM 접근이 명확하고 안정적
- **에러 알림**:
  - 상단 입력 중 검증 실패 → 즉시 alert 팝업 표시
  - 팝업 확인 후 상단으로 스크롤, 하단 에러 메시지도 표시
- **테스트**: `mvn test` 실행 결과 모든 테스트 통과 (11 tests, 0 failures)

### 5) 인간 검증 및 수정
- **AI 생성 코드 검증**:
  - 프리셋 핸들러: 1차 시도(카드 기반 선택)는 불안정함을 확인하고 2차 시도(data-index 직접 선택)로 개선
  - 에러 알림: alert가 사용자 흐름을 멈추지만, 필수 알림에는 가장 확실한 방법임을 확인
  - 입력 검증 강화: 각 검증 단계마다 focus 이동으로 UX 개선 확인
- **추가 수정**: 없음 (AI 제안이 요구사항을 충족함)
- **문서화**: `TROUBLESHOOTING.md`에 두 가지 문제(프리셋 DOM 선택, 에러 시인성)를 각각 항목 7, 8로 추가
  - 증상 → 원인 가설 → 조치 내용 → 결과 → 배운 점 형식으로 상세 기록

### 6) 배운 점
- **동적 DOM 요소 선택**: 
  - 중첩 선택(`parent.querySelector(child)`)보다 고유 식별자(`data-index`) 직접 선택이 더 안정적
  - 요소 존재 여부를 항상 체크하여 방어적으로 코딩해야 함
- **사용자 피드백 설계**:
  - 중요한 알림(에러, 경고)은 현재 뷰포트/스크롤 위치에 의존하지 않도록 설계
  - alert는 강제성이 있지만, 필수 알림에는 가장 확실한 방법
- **UX 테스트의 중요성**:
  - 실제 사용 시나리오를 테스트해야만 발견되는 문제(프리셋 미동작, 에러 미인지)가 존재
  - 개발자가 예상한 경로만 테스트하면 놓치기 쉬움

### 7) 평가 관점에서의 의미
- **기능 정확성**: 모든 모드에서 프리셋이 정상 동작하고, 에러 발생 시 사용자가 놓치지 않도록 개선
- **결과물 판단력**: 1차 AI 제안이 불충분함을 실제 테스트로 확인하고, 2차 개선 요청으로 해결
- **반복적 개선 능력**: 실사용 테스트 → 문제 발견 → 원인 분석 → 수정 → 재검증의 사이클을 명확히 기록
- **UX/UI 품질**: 가산점 항목인 "뛰어난 UX/UI 디자인"에 기여 (에러 피드백 개선)