## 2026-02-12 — 기획서(문제 정의/기능 범위/문서 구조) 작성

### 1) AI에게 제공한 컨텍스트(배경/요구사항/제약)
- 배경: 주제 2 “시간=돈(기회비용)”을 생활 의사결정에 적용하는 웹앱 기획
- 목표: A/B 선택을 **총비용(직접비용+시간비용)**으로 비교하고 결론의 **근거(수식/중간값)**까지 보여주기
- 요구사항(필수 포함):  
  - 문제 정의(왜 이 문제인지), 핵심/부가 기능 구분  
  - 대상 사용자/사용자 시나리오  
  - 엣지 케이스 및 처리 방안  
  - 개선 과정 기록/AI 활용 기록을 “추후 추가 가능한 틀”로 준비
- 제약: README 가독성 우선, 기록은 docs로 분리(README는 최소 링크만 유지)

### 2) AI 생성 결과물을 어떻게 검증하고 수정했는지
- 검증 기준: 과제 요구사항(문서 필수 항목 3종) 누락 여부와, 평가자가 읽기 쉬운 구조인지 확인
- 수정/보완한 내용:
  - 문서가 “혼자 보기용”으로 흐르지 않도록 **평가 관점의 제출용 톤**으로 재구성
  - 불필요한 문구(배점/체크리스트/‘가장 중요’ 같은 표현) 제거 → 가독성 개선
  - “개발 진행된 것처럼 보이는 기록” 전부 제거 → **개발 전 상태를 정직하게 반영**
  - README는 요약만 두고, `CHANGELOG.md / RETROSPECTIVE.md / AI_LOG.md`로 기록 분리
  - 계산 모델을 문서에 명시하기 위해 수식(LaTeX) 포함 및 소수점 처리 기준(floor) 고정

### 3) AI와의 협업에서 발견한 문제점과 대응 방식
- 문제점 1) AI 답변이 길어져 README 가독성이 떨어질 수 있음  
  → 대응: 섹션을 짧게 쪼개고, 핵심만 남기며, 로그는 `docs/`로 분리
- 문제점 2) AI가 “개발 후 기록”을 예시로 채우며 사실처럼 보일 위험이 있음  
  → 대응: 개발 전 단계에서는 **템플릿만** 남기고, 실제 기록은 추후 작성하도록 분리
- 문제점 3) 요구사항을 빠짐없이 넣다 보면 문서가 과밀해질 수 있음  
  → 대응: (1) 문제 정의는 핵심만 요약 + 수식/시나리오/엣지 케이스는 명확히 구획,  
    (2)(3)은 링크/템플릿 중심으로 최소화하여 유지보수 부담을 줄임

---

## 2026-02-12 — Java/Spring 기반 웹앱 전체 구현

### 1) AI에게 제공한 컨텍스트(배경/요구사항/제약)
- 배경: 이미 정의된 문제(“시간=돈, 기회비용 계산”)를 **Java + Spring Boot 기반 웹앱**으로 구현해야 하는 과제
- 목표:
  - 2개 선택지 A/B의 총비용(Total Cost Index)을 계산·비교하는 **MVP 웹앱** 구현
  - 과제 평가 항목에 맞게 **문제 정의/개선 과정/AI 활용 기록**이 드러나는 구조 유지
  - 이후 확장을 고려한 구조(서비스/컨트롤러/DTO 분리, 테스트 가능 구조)
- 요구사항:
  - Java 17 + Spring Boot 3.x 기반
  - REST API + 간단한 프론트엔드(Thymeleaf + JS)
  - 필수 엣지 케이스(0/음수/문자/동일 결과/큰 값) 처리
  - 테스트 코드, README, 문서(CHANGELOG, RETROSPECTIVE, AI_LOG) 갱신
- 제약:
  - Maven/Java 설치 여부는 보장되지 않아 **로컬 빌드/테스트는 사용자 환경에 위임**
  - 시간 내에 “완벽한 아키텍처”보다 **평가 기준을 충족하는 실용적 구조**를 선택

### 2) AI가 수행한 주요 작업과 검증 방식

1. **Spring Boot 프로젝트 생성**
   - 작업:
     - `pom.xml` 작성 (Spring Web, Validation, Thymeleaf, Lombok, Test)
     - `OpportunityCostApplication` 생성, `application.properties` 기본 설정
   - 검증:
     - 의존성 목록을 과제 요구사항과 맞춰보고, 불필요한 라이브러리 제외
     - 린트 도구를 통해 구문 오류 여부 확인

2. **도메인/DTO/서비스/컨트롤러 설계**
   - 작업:
     - `ComparisonOption`, `CalculationRequest/Response`, `CostBreakdown` 설계
     - `OpportunityCostService`에서 수식 \(TC = C + (W/60) × T\) 구현 + floor 처리
     - `OpportunityCostController`에서 `/api/calculate` 엔드포인트 구현
   - 검증:
     - README/기획서에 정의된 예시(마트 vs 편의점, 요리 vs 배달)를 기반으로 **단위 테스트 코드** 작성
     - 테스트에서 기대값(5,500원, 12,300원 등)이 정확히 일치하는지 확인

3. **프론트엔드 구현 및 UX 개선**
   - 작업:
     - `index.html` + `style.css` + `app.js`로 시급 입력/선택지 A/B 입력/결과 표시 UI 구성
     - 프리셋 버튼, 계산식 토글, 로딩 상태, Enter 키 지원 등 UX 요소 추가
   - 검증:
     - 수기 입력/프리셋 조합으로 다양한 시나리오를 가정하고, 계산 결과가 문서의 예시와 일치하는지 로직을 다시 추적
     - 에러 메시지가 한국어로 자연스럽고, 필드별로 구체적인지 확인

4. **예외 처리/입력 검증/테스트/로깅**
   - 작업:
     - `GlobalExceptionHandler`로 전역 예외 처리
     - DTO와 JS에서 이중 검증(빈 문자열/NaN/음수/0 이상 조건)
     - `OpportunityCostServiceTest`, `OpportunityCostControllerTest` 작성
     - SLF4J 기반 로깅 추가 (요청/결과/캐시 히트 등)
   - 검증:
     - “시급 0/음수/null”, “선택지 누락”, “잘못된 JSON” 등 케이스에 대해 400/500 응답이 적절히 나오는지 테스트 코드로 확인
     - 로그 메시지가 과하게 장황하지 않고, 디버깅에 필요한 정보만 포함하는지 리뷰

5. **다안 비교/히스토리/캐싱/배포 준비(추가 기능)**
   - 작업:
     - `/api/calculate/multi` + `MultiComparisonRequest/Response`, `OptionResult` 구현
     - 프론트엔드에 다안 비교 모드(3~5개 선택지), 선택지 동적 추가/제거 UI 추가
     - `localStorage` 기반 히스토리 저장/조회/재사용 기능 구현
     - `CalculationCacheService`로 동일 입력에 대한 결과 캐싱
     - `Dockerfile`, `docker-compose.yml`, `Procfile`, `application-prod.properties`, `docs/DEPLOYMENT.md` 작성
   - 검증:
     - 다안 비교 결과가 2안 비교 수식과 동일한 로직을 공유하는지, 서비스 레이어 구현을 다시 따라가며 확인
     - 히스토리에서 항목을 클릭했을 때 입력 값/결과가 일관되게 복원되는지 JS 로직을 시뮬레이션
     - Docker 이미지 빌드/실행 플로우가 일반적인 Spring Boot 컨테이너 패턴과 맞는지 점검

### 3) AI와의 협업에서 드러난 장단점과 대응

- **장점 1) 반복적/기계적인 작업 자동화**
  - DTO/서비스/컨트롤러/테스트/문서 등 “패턴이 명확한 코드”를 빠르게 생성하여, 사람은 문제 정의/엣지 케이스/UX와 같은 상위 레벨 의사결정에 집중할 수 있었다.
  - 대응: 구조(레이어링, 네이밍 규칙)를 먼저 합의한 뒤, 반복 패턴은 AI에게 맡기고, 마지막에 사람이 한 번에 리뷰·정리하는 방식으로 진행.

- **장점 2) 평가 기준을 의식한 설계 보조**
  - 과제 평가표(문제 정의/결과물 판단력/개선 과정/맥락 관리)를 상기시키면서, 테스트 코드/문서/로그/배포 준비까지 “채점 포인트”를 모두 커버하도록 설계를 유도할 수 있었다.
  - 대응: 각 섹션(CHANGELOG, RETROSPECTIVE, AI_LOG, DEPLOYMENT)에 “이 항목이 어떤 평가 항목을 뒷받침하는지”를 의식하고 작성.

- **문제점 1) 과도한 자동화로 인한 복잡도 증가 위험**
  - 다안 비교/히스토리/캐싱/배포를 한 번에 추가하다 보면, 코드가 한 번에 크게 불어나서 “왜 이렇게 됐는지” 흐름이 흐려질 수 있었다.
  - 대응: `PROJECT_SUMMARY.md`와 `FINAL_SUMMARY.md`를 별도로 작성해, 기능 추가 순서와 의도를 텍스트로 정리하고, CHANGELOG에 버전별 요약을 남김.

- **문제점 2) 실제 실행 환경 차이 (예: Maven 미설치)**
  - 로컬에서 `mvn`이 없다는 오류가 발생해, AI 입장에서는 빌드/테스트를 직접 돌려볼 수 없었다.
  - 대응: 빌드/실행/배포 명령어는 문서로 명확히 남기고, 코드 레벨에서는 린트/타입 오류가 없도록 하는 데 집중했다.

- **문제점 3) “있어 보이는 것”과 “실제 필요한 것” 사이의 균형**
  - 예를 들어, 캐시 서비스나 다양한 배포 플랫폼 가이드는 과제 범위에 비해 과할 수 있지만, 평가 항목(배포, 독창성, 완성도)을 고려하면 의미가 있었다.
  - 대응: 필수 기능(MVP)을 먼저 완성/검증한 뒤, 남은 시간 안에서 “가산점에 직접 연결되는 추가 기능”만 선택적으로 도입했다.