## 2026-02-12 — 기획서(문제 정의/기능 범위/문서 구조) 작성

### 1) AI에게 제공한 컨텍스트(배경/요구사항/제약)
- 배경: 주제 2 “시간=돈(기회비용)”을 생활 의사결정에 적용하는 웹앱 기획
- 목표: A/B 선택을 **총비용(직접비용+시간비용)**으로 비교하고 결론의 **근거(수식/중간값)**까지 보여주기
- 요구사항(필수 포함):  
  - 문제 정의(왜 이 문제인지), 핵심/부가 기능 구분  
  - 대상 사용자/사용자 시나리오  
  - 엣지 케이스 및 처리 방안  
  - 개선 과정 기록/AI 활용 기록을 “추후 추가 가능한 틀”로 준비
- 제약: README 가독성 우선, 기록은 docs로 분리(README는 최소 링크만 유지)

### 2) AI 생성 결과물을 어떻게 검증하고 수정했는지
- 검증 기준: 프로젝트 필수 문서(문제 정의, 개선 과정, AI 활용) 누락 여부와 가독성 확인
- 수정/보완한 내용:
  - 문서가 “혼자 보기용”으로 흐르지 않도록 **평가 관점의 제출용 톤**으로 재구성
  - 불필요한 문구(배점/체크리스트/‘가장 중요’ 같은 표현) 제거 → 가독성 개선
  - “개발 진행된 것처럼 보이는 기록” 전부 제거 → **개발 전 상태를 정직하게 반영**
  - README는 요약만 두고, `CHANGELOG.md / RETROSPECTIVE.md / AI_LOG.md`로 기록 분리
  - 계산 모델을 문서에 명시하기 위해 수식(LaTeX) 포함 및 소수점 처리 기준(floor) 고정

### 3) AI와의 협업에서 발견한 문제점과 대응 방식
- 문제점 1) AI 답변이 길어져 README 가독성이 떨어질 수 있음  
  → 대응: 섹션을 짧게 쪼개고, 핵심만 남기며, 로그는 `docs/`로 분리
- 문제점 2) AI가 “개발 후 기록”을 예시로 채우며 사실처럼 보일 위험이 있음  
  → 대응: 개발 전 단계에서는 **템플릿만** 남기고, 실제 기록은 추후 작성하도록 분리
- 문제점 3) 요구사항을 빠짐없이 넣다 보면 문서가 과밀해질 수 있음  
  → 대응: (1) 문제 정의는 핵심만 요약 + 수식/시나리오/엣지 케이스는 명확히 구획,  
    (2)(3)은 링크/템플릿 중심으로 최소화하여 유지보수 부담을 줄임

---

## 2026-02-12 — Java/Spring 기반 웹앱 전체 구현

### 1) AI에게 제공한 컨텍스트(배경/요구사항/제약)
- 배경: 이미 정의된 문제(“시간=돈, 기회비용 계산”)를 **Java + Spring Boot 기반 웹앱**으로 구현해야 하는 과제
- 목표:
  - 2개 선택지 A/B의 총비용(Total Cost Index)을 계산·비교하는 **MVP 웹앱** 구현
  - 핵심 가치인 **문제 정의/개선 과정/AI 활용 기록**이 명확히 드러나는 구조 유지
  - 이후 확장을 고려한 구조(서비스/컨트롤러/DTO 분리, 테스트 가능 구조)
- 요구사항:
  - Java 17 + Spring Boot 3.x 기반
  - REST API + 간단한 프론트엔드(Thymeleaf + JS)
  - 필수 엣지 케이스(0/음수/문자/동일 결과/큰 값) 처리
  - 테스트 코드, README, 문서(CHANGELOG, RETROSPECTIVE, AI_LOG) 갱신
- 제약:
  - Maven/Java 설치 여부는 보장되지 않아 **로컬 빌드/테스트는 사용자 환경에 위임**
  - 시간 내에 “완벽한 아키텍처”보다 **평가 기준을 충족하는 실용적 구조**를 선택

> **로그 운영 원칙**
> - Git 커밋 메시지는 “한 줄 요약” 수준으로만 사용하고,  
>   - 예) `feat: add multi comparison`, `fix: handle invalid json`  
> - 대신, **컨텍스트·요청·검증·수정·의미**는 이 `AI_LOG.md`와 `CHANGELOG.md`, `RETROSPECTIVE.md`에 자세히 기록해  
>   - 평가자가 “AI를 어떻게 활용했고, 사람이 어떤 결정을 내렸는지”를 문서만으로 추적할 수 있게 한다.

### 2) AI가 수행한 주요 작업과 검증 방식

1. **Spring Boot 프로젝트 생성**
   - 작업:
     - `pom.xml` 작성 (Spring Web, Validation, Thymeleaf, Lombok, Test)
     - `OpportunityCostApplication` 생성, `application.properties` 기본 설정
   - 검증:
     - 의존성 목록을 과제 요구사항과 맞춰보고, 불필요한 라이브러리 제외
     - 린트 도구를 통해 구문 오류 여부 확인

2. **도메인/DTO/서비스/컨트롤러 설계**
   - 작업:
     - `ComparisonOption`, `CalculationRequest/Response`, `CostBreakdown` 설계
     - `OpportunityCostService`에서 수식 \(TC = C + (W/60) × T\) 구현 + floor 처리
     - `OpportunityCostController`에서 `/api/calculate` 엔드포인트 구현
   - 검증:
     - README/기획서에 정의된 예시(마트 vs 편의점, 요리 vs 배달)를 기반으로 **단위 테스트 코드** 작성
     - 테스트에서 기대값(5,500원, 12,300원 등)이 정확히 일치하는지 확인

3. **프론트엔드 구현 및 UX 개선**
   - 작업:
     - `index.html` + `style.css` + `app.js`로 시급 입력/선택지 A/B 입력/결과 표시 UI 구성
     - 프리셋 버튼, 계산식 토글, 로딩 상태, Enter 키 지원 등 UX 요소 추가
   - 검증:
     - 수기 입력/프리셋 조합으로 다양한 시나리오를 가정하고, 계산 결과가 문서의 예시와 일치하는지 로직을 다시 추적
     - 에러 메시지가 한국어로 자연스럽고, 필드별로 구체적인지 확인

4. **예외 처리/입력 검증/테스트/로깅**
   - 작업:
     - `GlobalExceptionHandler`로 전역 예외 처리
     - DTO와 JS에서 이중 검증(빈 문자열/NaN/음수/0 이상 조건)
     - `OpportunityCostServiceTest`, `OpportunityCostControllerTest` 작성
     - SLF4J 기반 로깅 추가 (요청/결과/캐시 히트 등)
   - 검증:
     - “시급 0/음수/null”, “선택지 누락”, “잘못된 JSON” 등 케이스에 대해 400/500 응답이 적절히 나오는지 테스트 코드로 확인
     - 로그 메시지가 과하게 장황하지 않고, 디버깅에 필요한 정보만 포함하는지 리뷰

5. **다안 비교/히스토리/캐싱/배포 준비(추가 기능)**
   - 작업:
     - `/api/calculate/multi` + `MultiComparisonRequest/Response`, `OptionResult` 구현
     - 프론트엔드에 다안 비교 모드(3~5개 선택지), 선택지 동적 추가/제거 UI 추가
     - `localStorage` 기반 히스토리 저장/조회/재사용 기능 구현
     - `CalculationCacheService`로 동일 입력에 대한 결과 캐싱
     - `Dockerfile`, `docker-compose.yml`, `Procfile`, `application-prod.properties`, `docs/DEPLOYMENT.md` 작성
   - 검증:
     - 다안 비교 결과가 2안 비교 수식과 동일한 로직을 공유하는지, 서비스 레이어 구현을 다시 따라가며 확인
     - 히스토리에서 항목을 클릭했을 때 입력 값/결과가 일관되게 복원되는지 JS 로직을 시뮬레이션
     - Docker 이미지 빌드/실행 플로우가 일반적인 Spring Boot 컨테이너 패턴과 맞는지 점검

### 3) AI와의 협업에서 드러난 장단점과 대응

- **장점 1) 반복적/기계적인 작업 자동화**
  - DTO/서비스/컨트롤러/테스트/문서 등 “패턴이 명확한 코드”를 빠르게 생성하여, 사람은 문제 정의/엣지 케이스/UX와 같은 상위 레벨 의사결정에 집중할 수 있었다.
  - 대응: 구조(레이어링, 네이밍 규칙)를 먼저 합의한 뒤, 반복 패턴은 AI에게 맡기고, 마지막에 사람이 한 번에 리뷰·정리하는 방식으로 진행.

- **장점 2) 평가 기준을 의식한 설계 보조**
  - 과제 평가표(문제 정의/결과물 판단력/개선 과정/맥락 관리)를 상기시키면서, 테스트 코드/문서/로그/배포 준비까지 “채점 포인트”를 모두 커버하도록 설계를 유도할 수 있었다.
  - 대응: 각 섹션(CHANGELOG, RETROSPECTIVE, AI_LOG, DEPLOYMENT)에 “이 항목이 어떤 평가 항목을 뒷받침하는지”를 의식하고 작성.

- **문제점 1) 과도한 자동화로 인한 복잡도 증가 위험**
  - 다안 비교/히스토리/캐싱/배포를 한 번에 추가하다 보면, 코드가 한 번에 크게 불어나서 “왜 이렇게 됐는지” 흐름이 흐려질 수 있었다.
  - 대응: `PROJECT_SUMMARY.md`와 `FINAL_SUMMARY.md`를 별도로 작성해, 기능 추가 순서와 의도를 텍스트로 정리하고, CHANGELOG에 버전별 요약을 남김.

- **문제점 2) 실제 실행 환경 차이 (예: Maven 미설치)**
  - 로컬에서 `mvn`이 없다는 오류가 발생해, AI 입장에서는 빌드/테스트를 직접 돌려볼 수 없었다.
  - 대응: 빌드/실행/배포 명령어는 문서로 명확히 남기고, 코드 레벨에서는 린트/타입 오류가 없도록 하는 데 집중했다.

- **문제점 3) “있어 보이는 것”과 “실제 필요한 것” 사이의 균형**
  - 예를 들어, 캐시 서비스나 다양한 배포 플랫폼 가이드는 과제 범위에 비해 과할 수 있지만, 평가 항목(배포, 독창성, 완성도)을 고려하면 의미가 있었다.
  - 대응: 필수 기능(MVP)을 먼저 완성/검증한 뒤, 남은 시간 안에서 “가산점에 직접 연결되는 추가 기능”만 선택적으로 도입했다.

---

## 2026-02-12 — 잘못된 JSON 요청 처리 개선

### 1) 컨텍스트
- 상황: `OpportunityCostControllerTest.testCalculate_InvalidJson` 테스트에서, 잘못된 JSON(`{ invalid json }`) 요청에 대해 **400이 아니라 500**이 반환되며 테스트가 실패함.
- 기대: 잘못된 요청 본문(JSON 파싱 오류)은 **클라이언트 잘못**이므로, 500이 아니라 **400 Bad Request**가 되어야 함.

### 2) AI에게 요청한 작업
- `HttpMessageNotReadableException`을 전역 예외 처리기에서 **별도로 처리하여 400으로 매핑**해 달라고 요청.
- 테스트 로그에 나온 스택 트레이스를 바탕으로, 어느 계층에서 예외가 발생하는지(요청 바디 → Jackson 파싱 → Spring MVC) 분석해 반영.

### 3) 적용된 변경 사항
- `GlobalExceptionHandler` 수정:
  - `@ExceptionHandler(HttpMessageNotReadableException.class)` 추가 → 잘못된 JSON 요청 시 400 응답 반환.
  - 기존 `@ExceptionHandler(Exception.class)`는 “마지막 방어선”으로 두어, 예상치 못한 나머지 예외만 500으로 처리.
- 테스트 코드:
  - `testCalculate_InvalidJson`는 그대로 두고, 전역 예외 처리기를 통해 기대 상태 코드(400)를 만족하도록 구현을 조정.

### 4) 검증
- `mvn test` 실행 결과:
  - `OpportunityCostControllerTest.testCalculate_InvalidJson`: 기대 상태 코드 400, 실제 400으로 통과.
  - 전체 테스트 요약: **Failures 0, Errors 0**.
- 로그:
  - 이전에는 `Exception` 핸들러에서 500으로 처리되던 스택 트레이스가,
  - 변경 후에는 `요청 본문 파싱 오류` 로그 + 400 응답으로 정리됨.

### 5) 실무적 의미
- **API 사용성**: 잘못된 JSON 입력에 대해 올바른 HTTP 상태 코드와 명확한 에러 메시지를 반환하여 클라이언트 디버깅 용이
- **품질 개선 과정**: 실제 테스트 실패 로그를 기반으로 예외 처리를 보완하고, 테스트를 다시 통과시키는 점진적 개선
- **일관성**: 전역 예외 처리기의 역할과, 어떤 예외를 어떤 상태 코드로 매핑할지에 대한 기준을 문서와 코드 양쪽에서 일관되게 유지

---

## 2026-02-12 — 다안 비교 프리셋 동작 및 에러 알림 UX 개선

### 1) 컨텍스트
- **상황**: 실제 사용 테스트에서 두 가지 UX 문제 발견:
  1. 다안 비교 모드로 전환 후 "예시 시나리오" 프리셋 버튼을 눌러도 입력 카드에 값이 채워지지 않음
  2. 입력 검증 실패 시 에러 메시지가 화면 하단에만 표시되어, 상단 입력 중인 사용자가 에러를 놓치기 쉬움
- **기대**: 
  1. 프리셋 버튼이 모든 모드(2안 비교, 다안 비교)에서 정상 동작
  2. 에러 발생 시 사용자 위치와 관계없이 즉시 인지 가능

### 2) AI에게 요청한 작업
- **1차 수정 (프리셋 문제)**: 
  - 프리셋 핸들러에서 `currentMode`에 따라 분기하여 다안 비교 모드일 때도 입력 카드에 값을 채워달라고 요청
  - `document.querySelectorAll('.multi-option-card')`로 카드를 찾은 뒤, 내부 input 요소를 선택하여 값 주입
- **2차 수정 (프리셋 문제 재발)**:
  - 1차 수정 후에도 여전히 동작하지 않는다는 피드백 → DOM 선택 방식 재검토 요청
  - `data-index` 속성을 직접 사용하는 방식으로 변경 요청
- **에러 알림 개선**:
  - `showError()` 함수를 alert 팝업으로 개선하고, 상단 스크롤도 추가 요청
  - 다안 비교 계산 시 입력 검증 강화 요청 (시급, 각 선택지별 상세 검증)

### 3) 적용된 변경 사항
- **프리셋 핸들러 수정 (`src/main/resources/static/js/app.js`)**:
  - 1차: 카드 기반 선택 → 여전히 불안정
  - 2차: `document.querySelector('.multi-time-input[data-index="0"]')` 방식으로 직접 선택
  - 입력 필드를 찾지 못하면 "다안 비교 모드로 먼저 전환해주세요" 안내
- **에러 알림 개선 (`showError()` 함수)**:
  - `alert()` 팝업 추가 → 사용자가 어디에 있든 즉시 알림
  - 하단 에러 메시지 영역도 유지 (참고용)
  - `window.scrollTo({ top: 0, behavior: 'smooth' })` 추가
- **다안 비교 입력 검증 강화 (`calculateMulti()` 함수)**:
  - 시급 검증: 빈 문자열, NaN, 음수, 큰 값 경고
  - 각 선택지별 검증: 입력 필드 존재 여부, 빈 값, 숫자 변환, 범위 체크
  - 검증 실패 시 해당 입력 필드로 `focus()` 이동

### 4) 검증
- **프리셋 동작**: 
  - 다안 비교 모드 전환 → 프리셋 버튼 클릭 → 첫 번째/두 번째 카드에 값 정상 채워짐 확인
  - data-index 기반 선택으로 DOM 접근이 명확하고 안정적
- **에러 알림**:
  - 상단 입력 중 검증 실패 → 즉시 alert 팝업 표시
  - 팝업 확인 후 상단으로 스크롤, 하단 에러 메시지도 표시
- **테스트**: `mvn test` 실행 결과 모든 테스트 통과 (11 tests, 0 failures)

### 5) 인간 검증 및 수정
- **AI 생성 코드 검증**:
  - 프리셋 핸들러: 1차 시도(카드 기반 선택)는 불안정함을 확인하고 2차 시도(data-index 직접 선택)로 개선
  - 에러 알림: alert가 사용자 흐름을 멈추지만, 필수 알림에는 가장 확실한 방법임을 확인
  - 입력 검증 강화: 각 검증 단계마다 focus 이동으로 UX 개선 확인
- **추가 수정**: 없음 (AI 제안이 요구사항을 충족함)
- **문서화**: `TROUBLESHOOTING.md`에 두 가지 문제(프리셋 DOM 선택, 에러 시인성)를 각각 항목 7, 8로 추가
  - 증상 → 원인 가설 → 조치 내용 → 결과 → 배운 점 형식으로 상세 기록

### 6) 배운 점
- **동적 DOM 요소 선택**: 
  - 중첩 선택(`parent.querySelector(child)`)보다 고유 식별자(`data-index`) 직접 선택이 더 안정적
  - 요소 존재 여부를 항상 체크하여 방어적으로 코딩해야 함
- **사용자 피드백 설계**:
  - 중요한 알림(에러, 경고)은 현재 뷰포트/스크롤 위치에 의존하지 않도록 설계
  - alert는 강제성이 있지만, 필수 알림에는 가장 확실한 방법
- **UX 테스트의 중요성**:
  - 실제 사용 시나리오를 테스트해야만 발견되는 문제(프리셋 미동작, 에러 미인지)가 존재
  - 개발자가 예상한 경로만 테스트하면 놓치기 쉬움

### 7) 평가 관점에서의 의미
- **기능 완성도**: 모든 모드에서 프리셋이 정상 동작하고, 에러 발생 시 사용자가 놓치지 않도록 개선
- **품질 개선**: 1차 AI 제안의 불충분함을 실제 테스트로 확인하고, 2차 개선 요청으로 해결
- **점진적 개선**: 실사용 테스트 → 문제 발견 → 원인 분석 → 수정 → 재검증의 사이클을 명확히 기록
- **사용자 경험**: 에러 피드백 개선으로 사용자 흐름을 방해하지 않으면서도 문제를 명확히 전달

---

## 2026-02-12 — 다안 비교 프리셋 완전 지원 (3~5개 선택지)

### 1) 컨텍스트
- **상황**: 
  - 실제 사용 테스트에서 다안 비교 모드의 프리셋이 부분적으로만 동작
  - 선택지 A, B만 채워지고, C, D, E는 빈 값으로 남음
  - 사용자가 수동으로 나머지 선택지를 입력해야 하므로 프리셋의 의미가 반감됨
- **사용자 피드백**:
  - "지금 일부만 동작을 해. 선택지 a b 의 값만 채워지고 선택지가 3개 이상일 때는 c 부터 채워지지 않아."
  - "다안 비교 일 때, 예시 시나리오가 작동하도록 수정해. 기본 값이 아닌, a와 b 처럼 프리셋 값으로 동작하도록해"
- **추가 피드백**:
  - alert 팝업이 너무 침습적이므로 제거 요청
  - 하단 에러 메시지 영역과 자동 스크롤만 유지

### 2) AI에게 요청한 작업
- **프리셋 데이터 구조 확장**:
  - 기존 `optionA`, `optionB` 외에 `options` 배열 추가
  - 각 시나리오(마트, 요리, 배송)마다 의미 있는 5가지 옵션 정의
  - 하위 호환성 유지 (2안 비교 모드에서도 동작)
- **프리셋 핸들러 수정**:
  - 다안 비교 모드일 때 `multiOptionCount`만큼 반복하여 모든 선택지 채우기
  - `preset.options[i]`에서 순차적으로 값 가져오기
  - 데이터 없는 인덱스는 빈 값으로 유지하되 콘솔 경고
- **에러 알림 개선**:
  - alert 제거
  - 하단 에러 메시지 표시 + 상단 자동 스크롤만 유지

### 3) 적용된 변경 사항
- **프리셋 데이터 (`src/main/resources/static/js/app.js`)**:
  ```javascript
  const presets = {
      mart: {
          hourlyWage: 15000,
          options: [
              { timeMinutes: 10, directCost: 3000 },   // A: 가까운 마트
              { timeMinutes: 40, directCost: 2300 },   // B: 먼 마트
              { timeMinutes: 25, directCost: 2800 },   // C: 중간 거리 마트
              { timeMinutes: 5, directCost: 3500 },    // D: 편의점
              { timeMinutes: 60, directCost: 2000 }    // E: 대형마트
          ],
          optionA: { ... }, // 하위 호환성
          optionB: { ... }
      },
      // cooking, delivery도 동일하게 확장
  }
  ```
- **프리셋 핸들러**:
  - 다안 비교 모드: `for (let i = 0; i < multiOptionCount; i++)` 반복
  - 각 인덱스마다 `preset.options[i]`에서 값 가져와 채우기
  - 입력 필드 찾기: `document.querySelector(\`.multi-time-input[data-index="${i}"]\`)`
- **에러 알림 (`showError()` 함수)**:
  - alert 제거
  - 하단 에러 메시지 표시 + 상단 스크롤만 유지

### 4) 검증
- **프리셋 동작**:
  - 다안 비교 모드에서 선택지 3개, 4개, 5개 각각 테스트
  - 프리셋 버튼 클릭 시 모든 선택지가 의미 있는 값으로 채워짐
  - 즉시 "계산하기" 버튼 클릭 가능
- **에러 알림**:
  - alert 없이 하단 메시지와 자동 스크롤만 동작
  - 사용자 흐름을 방해하지 않으면서도 충분히 눈에 띔
- **콘솔 로그**:
  - "선택지 A 채움: {...}", "선택지 B 채움: {...}" 등
  - "프리셋 적용 완료: 5개 선택지 채움"

### 5) 인간 검증 및 수정
- **AI 생성 코드 검증**:
  - 프리셋 데이터: 각 시나리오마다 5가지 의미 있는 옵션 제공 확인
  - 프리셋 핸들러: 모든 선택지를 순차적으로 채우는 로직 동작 확인
  - 에러 알림: alert 제거 후 더 부드러운 UX 확인
- **추가 수정**: 없음 (AI 제안이 요구사항을 완전히 충족함)
- **문서화**: 
  - `TROUBLESHOOTING.md`에 항목 9 추가
  - `CHANGELOG.md`에 v1.2 버전 추가

### 6) 배운 점
- **데이터 구조의 확장성**:
  - 처음부터 확장 가능한 데이터 구조(`options` 배열)를 설계했다면 이런 문제를 방지할 수 있었음
  - 하위 호환성(`optionA`, `optionB`)과 확장성(배열) 모두 고려하는 것이 중요
- **UX의 반복적 개선**:
  - 1차: 프리셋 동작 안 함 → data-index 직접 선택으로 해결
  - 2차: A, B만 채워짐 → options 배열로 전체 선택지 지원
  - 3차: alert 너무 침습적 → 제거하고 스크롤만 유지
  - 실제 사용자 피드백을 통한 점진적 개선의 중요성
- **프리셋의 역할**:
  - 단순한 "예시"가 아니라 **사용자 경험의 핵심 요소**
  - 프리셋이 잘 작동하면 사용자가 즉시 기능을 체험하고 이해할 수 있음
  - 특히 다안 비교처럼 입력이 많은 기능에서는 더욱 중요

### 7) 실무적 의미
- **기능 완성도**: 다안 비교 모드의 모든 선택지가 프리셋으로 채워져 사용자가 즉시 테스트 가능
- **품질 개선**: 부분적으로만 동작하는 기능을 발견하고, 완전하게 수정
- **점진적 개선**: 사용자 피드백 3회 반영 → 각 단계마다 문제 해결 → 최종적으로 완성도 높은 UX 달성
- **사용자 경험**: 프리셋 완전 지원으로 학습 곡선 감소 + 비침습적 에러 알림으로 흐름 유지

---

## 2026-02-12 — 프로젝트 완성도 향상 (사용자 피드백 → 테스트 → 문서 강화)

### 1) 컨텍스트 제공
**요청**: "프로젝트를 체계적으로 개선하자. 나도 직접 참여할 수 있는 역할을 부여해줘."
**배경**:
- 사용자가 프로젝트의 완성도를 높이기 위해 체계적 개선 요청
- 7개 개선 영역 식별: ① 배포 준비, ② 사용자 시나리오 검증, ③ 테스트 커버리지, ④ 엣지 케이스, ⑤ 우선순위 조정, ⑥ 문제 측정 가능성, ⑦ AI 활용 기록, ⑧ 코드 구조
- **핵심**: 사용자가 직접 참여할 수 있는 역할 부여 필요

**AI에게 제공한 정보**:
- 프로젝트 목표 및 완성도 기준
- 기존 프로젝트 문서 (README, RETROSPECTIVE, AI_LOG, TROUBLESHOOTING, CHANGELOG)
- 사용자 테스트 결과 (5개 시나리오 × 체크리스트 + 기록)

### 2) AI 제안 내용
**우선순위 결정 (AI의 판단)**:
1. **사용자 시나리오 검증** - 사용자 필수 참여
   - 실제 웹앱 사용 + 5가지 시나리오 테스트
   - 구조화된 체크리스트 제공
   - **이유**: 실제 사용자 경험은 AI가 대신할 수 없음
2. **테스트 커버리지 강화** - AI 단독 수행
   - 17개 테스트로 확장 (기존 11개)
   - **이유**: 신뢰성 확보를 위한 체계적 검증 필요
3. **엣지 케이스 완전 구현** - AI 단독 수행
   - 단위 변환 가이드 추가
   - **이유**: 실제 사용 시 혼란 방지
4. **우선순위 조정 구체화** - AI 단독 수행
   - RETROSPECTIVE.md 대폭 강화
   - **이유**: 의사결정 과정의 투명성 확보
5. **문제 측정 가능성 개선** - AI 단독 수행
   - 측정 가능한 지표 추가
   - **이유**: 성공 기준을 명확히 정의
6. **AI 활용 기록 심화** - AI 단독 수행
   - **이유**: AI 협업 경험의 체계적 기록
7. **코드 구조 강화** - AI 단독 수행
   - **이유**: 확장성과 유지보수성 증명

### 3) AI 생성 결과물
**1단계: 사용자 테스트 가이드 작성**
- 5가지 시나리오별 체크리스트
- 기록 템플릿 (✅ 확인/💭 기록)
- 구체적 행동 지시 (클릭, 입력, 확인 항목)

**2단계: 사용자 피드백 즉시 반영**
- **계산식 상세화**: `displayResult()` 함수 개선
  ```javascript
  // AS-IS
  document.getElementById('formulaText').textContent = result.formula;
  
  // TO-BE
  const detailedFormula = `📊 계산 공식: ...
  【선택지 A 상세 계산】
  ・직접 비용: ${result.optionA.directCost.toLocaleString()}원
  ・시간 비용: ${perMinute}원/분 × ${timeA}분 = ${result.optionA.timeCost.toLocaleString()}원
  ・총 비용: ... = ${result.optionA.totalCost.toLocaleString()}원`;
  ```
- **히스토리 정책 변경**: `localStorage` → `sessionStorage`
  - 사용자: "서버 재시작 시 이전 기록이 남아서 혼란"
  - AI: 세션 기반으로 변경하여 탭 닫으면 자동 삭제
- **제목/설명 개선**: "💰 시간과 돈, 함께 계산하기" + 설명 추가
- **단위 변환 가이드**: 접기/펴기 가능한 `<details>` 추가

**3단계: 테스트 강화 (11개 → 17개)**
- 매우 큰 시급 입력 (1,500,000원)
- 매우 긴 시간 입력 (500분)
- 캐시 동작 검증 (동일 요청 2회)
- 음수 시간/비용 입력 검증
- 매우 큰 시급으로 계산 성공

**4단계: 문서 강화**
- **RETROSPECTIVE.md**: 실제 의사결정 예시, 막혔던 부분, trade-off 분석
- **README.md**: 측정 가능한 지표 6개 + 목표 vs 실제 테이블
- **AI_LOG.md**: 이 섹션 추가

### 4) 검증
**사용자 피드백 검증**:
- 계산식: "각 선택지별로 계산이 상세하게 나오는 것이 더 좋을 것 같다" ✅
- 히스토리: "서버를 새로 가동할 시 기록이 제거되었으면" ✅
- 디자인: "제목이나 디자인이 좀 더 사용자에게 다가갔으면" ✅
- 단위: "1시간 20분 = ?분" 혼동 방지 ✅

**테스트 검증**:
- 17/17 테스트 통과 ✅
- 빌드 성공 (Maven) ✅

**문서 검증**:
- 프로젝트 목표 대조 (4개 핵심 역량 × 세부 항목)
- 개선 영역 체계적 해소

### 5) 인간 검증 및 수정
**사용자 직접 수행**:
- 5가지 시나리오 테스트 완료
- 4건의 구체적 피드백 제공
- "예" 또는 구체적 개선 제안

**AI 수정**:
- 모든 피드백을 즉시 반영
- Git 커밋 4회 (feat, fix, docs 분리)
- 재테스트 요청 및 서버 재시작

**추가 수정**: 없음 (사용자 피드백이 명확하고, AI 구현이 요구사항을 완전히 충족)

### 6) 배운 점
**사용자 참여의 가치**:
- AI는 "예상되는 사용자 경험"을 모델링할 수 있지만, **실제 사용자의 직접적 경험**을 대신할 수 없음
- 사용자 테스트를 통해 발견된 4가지 개선점은 모두 AI가 스스로 발견하기 어려운 것들이었음:
  1. 계산식의 상세도 부족 (주관적 판단)
  2. 히스토리 정책의 혼란 (컨텍스트 전환 경험)
  3. 제목/디자인의 평범함 (감성적 평가)
  4. 단위 혼동 (실제 사용자의 인지 부하)

**체계적 접근의 효과**:
- 프로젝트 목표를 먼저 분석 → 개선 영역 식별 → 우선순위 정렬 → 체계적 해소
- "어디서부터 시작할지 모르겠다"는 상황을 **구조화된 작업 목록**으로 변환
- 각 단계마다 TODO 관리 → 진행 상황 투명화

**AI의 강점 활용**:
- **반복 작업 자동화**: 테스트 코드 17개 작성 (기존 11개 기반으로 패턴 확장)
- **문서 일관성 유지**: 4개 문서 (CHANGELOG, RETROSPECTIVE, AI_LOG, README)를 일관된 구조로 업데이트
- **즉시 실행**: 사용자 피드백 → 코드 수정 → 테스트 → 커밋 (5분 내 완료)

**인간의 강점 활용**:
- **실제 경험**: 웹앱을 직접 사용하며 느낀 혼란, 기대, 만족
- **가치 판단**: "이 정도면 충분하다" 또는 "더 개선이 필요하다"
- **우선순위 결정**: "배포는 나중에, 사용성 먼저"

### 7) 실무적 의미
- **문제 정의 명확화**: 측정 가능한 지표 6개 + 목표 vs 실제 테이블로 성공 기준 구체화
- **품질 보증**: 17개 테스트 통과 + 사용자 피드백 4건 반영으로 신뢰성 확보
- **투명한 개선 과정**: 우선순위 조정 기록 + 막힌 부분 해결 과정으로 의사결정 추적 가능
- **프로젝트 문서화**: AI 활용 기록 심화 + 코드 구조 문서화로 유지보수성 향상
- **사용자 중심 개선**: 17개 테스트로 안정성 확보, 사용자 피드백 기반 UX 개선으로 실사용성 향상

**총 개선 효과**: 7개 영역 체계적 개선으로 프로젝트 완성도 대폭 향상